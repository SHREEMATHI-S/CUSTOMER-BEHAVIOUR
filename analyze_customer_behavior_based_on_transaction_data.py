# -*- coding: utf-8 -*-
"""Analyze customer behavior based on transaction data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l_XVtKBCEFaHHjwDfIUPwC60TreyXbsa
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

import pandas as pd
import matplotlib.pyplot as plt
from google.colab import files
uploads1 = files.upload()

import pandas as pd
import io

# Load each file into a pandas DataFrame using the uploaded content
Ideal_data = pd.read_csv(io.BytesIO(uploads1['Hackathon_Ideal_Data.csv']))

# Now you can call head() on the DataFrame
print("\nIdeal Data:")
print(Ideal_data.head())

uploads2 = files.upload()

import pandas as pd
import io

# Load each file into a pandas DataFrame using the uploaded content
mapping_data = pd.read_csv(io.BytesIO(uploads2['Hackathon_Mapping_File.csv']))

# Now you can call head() on the DataFrame
print("\nMapping Data:")
print(mapping_data.head())

uploads3 = files.upload()

import pandas as pd
import io

# Load each file into a pandas DataFrame using the uploaded content
Validation_data = pd.read_csv(io.BytesIO(uploads3['Hackathon_Validation_Data.csv']))

# Now you can call head() on the DataFrame
print("\n Validation Data:")
print(Validation_data.head())

uploads4 = files.upload()

import pandas as pd
import io

# Load each file into a pandas DataFrame using the uploaded content
Working_data = pd.read_csv(io.BytesIO(uploads4['Hackathon_Working_Data.csv']))

# Now you can call head() on the DataFrame
print("\n Working Data:")
print(Working_data.head())

print(type(uploads1))  # Check the type of uploads1

print(uploads1.keys())  # This will show the keys in the dictionary

print("\n IDEAL DATA")
print(Ideal_data.head())
print("\n MAPPING DATA")
print(mapping_data.head())
print("\n VALIDATION DATA")
print(Validation_data.head())
print("\n WORKING DATA")
print(Working_data.head())

import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
ideal_data = pd.read_csv(io.BytesIO(uploads1['Hackathon_Ideal_Data.csv']))

working_data = pd.read_csv(io.BytesIO(uploads4['Hackathon_Working_Data.csv']))

imputer = SimpleImputer(strategy="mean")
ideal_data_filled = imputer.fit_transform(ideal_data.select_dtypes(include=['int', 'float']))
ideal_data_filled = pd.DataFrame(ideal_data_filled, columns=ideal_data.select_dtypes(include=['int', 'float']).columns)

# Data cleaning, normalization, and standardization
scaler = StandardScaler()
working_data_scaled = scaler.fit_transform(working_data.select_dtypes(include=['int', 'float']))
working_data_scaled = pd.DataFrame(working_data_scaled, columns=working_data.select_dtypes(include=['int', 'float']).columns)

# Convert categorical variables into numerical representations (One-hot encoding)
encoder = OneHotEncoder()
ideal_data_encoded = pd.get_dummies(ideal_data.select_dtypes(include=['object']))
ideal_data_preprocessed = pd.concat([ideal_data_filled, ideal_data_encoded], axis=1)

# Print preprocessed data
print("Ideal Data after preprocessing:")
print(ideal_data_preprocessed.head())
print("\nWorking Data after scaling:")
print(working_data_scaled.head())

working_data_scaled.head()

import seaborn as sns
import matplotlib.pyplot as plt

# Visualize the distribution of key variables
plt.figure(figsize=(12, 6))
sns.histplot(data=working_data_scaled['BILL_AMT'], bins=30, kde=True, color='blue')
plt.title('Distribution of Bill Amount')
plt.xlabel('Bill Amount')
plt.ylabel('Frequency')
plt.show()

# Explore correlations between different variables
plt.figure(figsize=(12, 8))
correlation_matrix = working_data_scaled.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

plt.figure(figsize=(12, 6))
sns.lineplot(data=working_data_scaled, x='DAY', y='BILL_AMT')
plt.title('Trends in Bill Amount over Time')
plt.xlabel('Day')
plt.ylabel('Bill Amount')
plt.show()

# Visualize the distribution of a categorical variable using a bar chart
plt.figure(figsize=(14, 8))  # Increase the size of the chart
sns.countplot(data=ideal_data, y='GRP', order=ideal_data['GRP'].value_counts().index, palette='pastel')
plt.title('Distribution of Product Groups', fontsize=16)  # Increase the title font size
plt.xlabel('Count', fontsize=14)  # Increase the x-axis label font size
plt.ylabel('Product Group', fontsize=14)  # Increase the y-axis label font size
plt.xticks(fontsize=5)  # Decrease the tick label font size
plt.yticks(fontsize=5)  # Decrease the tick label font size
plt.show()

# Scatter plot to explore the relationship between two numerical variables
plt.figure(figsize=(10, 6))
sns.scatterplot(data=working_data_scaled, x='QTY', y='VALUE', color='skyblue')
plt.title('Relationship between Quantity and Value', fontsize=16)
plt.xlabel('Quantity', fontsize=14)
plt.ylabel('Value', fontsize=14)
plt.show()

# Box plot to visualize the distribution of a numerical variable across different categories
plt.figure(figsize=(12, 8))
sns.boxplot(data=ideal_data, x='MONTH', y='VALUE', palette='pastel')
plt.title('Distribution of Value across Months', fontsize=16)
plt.xlabel('Month', fontsize=14)
plt.ylabel('Value', fontsize=14)
plt.show()

working_data_scaled.head()

working_data_scaled.columns

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, GRU, Conv1D, Flatten

# Assuming you have loaded the scaled data into a DataFrame named 'working_data_scaled'

# Assuming the target variable is stored in the 'PRICE' column of your DataFrame
target_variable = working_data_scaled['PRICE']

# Splitting the data into features (X) and target variable (y)
X = working_data_scaled.drop(columns=['PRICE'])  # Exclude target variable from features
y = target_variable

# Reshape X for Conv1D input
X_cnn = X.values.reshape(X.shape[0], X.shape[1], 1)

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_cnn, y, test_size=0.2, random_state=42)
# Define and compile the models
gru_model = Sequential([
    GRU(units=64, input_shape=(X_train.shape[1], 1)),
    Dense(units=1)
])
gru_model.compile(optimizer='adam', loss='mse')

lstm_model = Sequential([
    LSTM(units=64, input_shape=(X_train.shape[1], 1)),
    Dense(units=1)
])
lstm_model.compile(optimizer='adam', loss='mse')

cnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),
    Flatten(),
    Dense(units=1)
])
cnn_model.compile(optimizer='adam', loss='mse')
# Train the models
gru_model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)
lstm_model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)
cnn_model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)

# Evaluate the models
gru_loss = gru_model.evaluate(X_test, y_test)
lstm_loss = lstm_model.evaluate(X_test, y_test)
cnn_loss = cnn_model.evaluate(X_test, y_test)

# Print the evaluation results
print("GRU Model Loss:", gru_loss)
print("LSTM Model Loss:", lstm_loss)
print("CNN Model Loss:", cnn_loss)

y_pred = gru_model.predict(X_test)
y_true = y_test

# Visualize Model Outputs

plt.figure(figsize=(10, 6))
plt.plot(y_true, label='Actual')
plt.plot(y_pred, label='Predicted')
plt.title('Model Predictions vs. Actual Values')
plt.xlabel('Samples')
plt.ylabel('Price')
plt.legend()
plt.show()

# Example: Scatter plot of predicted vs. actual values
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_true.values.flatten(), y=y_pred.flatten())
plt.title('Scatter Plot of Predicted vs. Actual Values')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.show()

#  Extract Actionable Insights
# Derive actionable insights from the model predictions
# Example: Calculate the mean absolute error (MAE) to evaluate model performance
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_true, y_pred)
print("Mean Absolute Error (MAE):", mae)

#  Identify Areas for Improvement
# Evaluate model performance and identify areas for improvement
# Example: Plot the distribution of prediction errors
errors = (y_true.squeeze() - y_pred.squeeze())
plt.figure(figsize=(8, 6))
sns.histplot(errors, kde=True)
plt.title('Distribution of Prediction Errors')
plt.xlabel('Prediction Error')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt

# Plotting actual vs predicted prices
plt.figure(figsize=(10, 6))
plt.scatter(y_true, y_pred, color='blue', alpha=0.5)
plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], color='red', linestyle='--')  # Plotting the diagonal line
plt.title('Actual vs Predicted Prices')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.show()

working_data_scaled.head()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import load_model

# Assuming 'working_data_scaled' contains your scaled dataset
df = working_data_scaled

# Assume 'X' contains the features used for prediction
X = df.drop(columns=['PRICE'])  # Exclude the target variable
y_true = df['PRICE']  # Actual prices from the dataset

# Reshape X to match the expected input shape of the GRU model
# Model expects (samples, timesteps, features)
# Here, timesteps = 4 and features = 1
X = np.reshape(X.values, (X.shape[0], 4, 1))  # Assuming X has 4 features and reshaping to (samples, 4, 1)

# Ensure the model is loaded correctly (Replace 'gru_model' with your actual model)
model = gru_model  # Ensure that 'gru_model' is the trained GRU model you want to use

# Print the model's expected input shape
print(f"Model input shape: {model.input_shape}")

# Verify that X matches the expected input shape
print(f"Shape of X before prediction: {X.shape}")

# Use the model to make predictions
try:
    y_pred = model.predict(X)
    # Flatten y_pred to match the shape of y_true
    y_pred = y_pred.flatten()
except Exception as e:
    print(f"Error during prediction: {e}")
    raise

# Check the shapes of y_true and y_pred
print(f"Shape of y_true: {y_true.shape}")
print(f"Shape of y_pred: {y_pred.shape}")

# Ensure both y_true and y_pred are of the same shape before calculating RMSE
if y_true.shape != y_pred.shape:
    raise ValueError(f"Shape mismatch: y_true shape {y_true.shape} and y_pred shape {y_pred.shape}")

# Calculate RMSE (Root Mean Squared Error)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
print("Root Mean Squared Error (RMSE):", rmse)

# Plotting the actual vs predicted prices
plt.figure(figsize=(10, 6))
bar_width = 0.35  # Define the bar width
plt.bar(np.arange(len(y_true)), y_true, width=bar_width, label='Actual Price', color='blue')
plt.bar(np.arange(len(y_pred)) + bar_width, y_pred, width=bar_width, label='Predicted Price', color='orange')
plt.xlabel('Sample Index')
plt.ylabel('Price')
plt.title('Comparison of Actual and Predicted Prices')
plt.legend()
plt.show()

import io
import pandas as pd

# Assuming uploads3 is a dictionary-like object with file content
validation_data_file = pd.read_csv(io.BytesIO(uploads3['Hackathon_Validation_Data.csv']))

# Directly use the DataFrame
validate_df = validation_data_file
validate_df.head()

gru_model.summary()

print("Shape of ID column:", validate_df['ID'].shape)

# Check the shape of y_pred
print("Shape of y_pred:", y_pred.shape)

# Filter y_pred based on the length of validate_df['ID']
y_pred_filtered = y_pred[:len(validate_df['ID'])]

# Convert predictions to integers and ensure non-negativity
y_pred_int = np.round(y_pred_filtered).astype(int)
y_pred_int[y_pred_int < 0] = 0

# Create the DataFrame for submission
submission_df = pd.DataFrame({'ID': validate_df['ID'], 'TOTALVALUE': y_pred_int.flatten()})

submission_df.head()

# Save the DataFrame to a CSV file
submission_df.to_csv('submission.csv', index=False)
print("DONE!!")